{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJXc9drhjL_B"
   },
   "source": [
    "| | |\n",
    "|:---:|:---|\n",
    "| <img src=\"https://drive.google.com/uc?export=view&id=1OaJVnbVa6RHE5tEl1e94s2B3L8BsO31K\" width=\"100\"/> |  <h1><b>Introduction to Language Models: Preprocessing, Modelling, and NLP</b>🙊</h1>|\n",
    "\n",
    "---\n",
    "\n",
    "**Instructor:**  \n",
    "Pavlos Protopapas  \n",
    "\n",
    "**Teaching Team:**  \n",
    "Nawang Thinley Bhutia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4rOU-TBg1gT"
   },
   "source": [
    "In this notebook, we will go over some of the fundamental concepts in the realm of language models, including basic preprocessing techniques, basic sentiment analysis, modelling, and a few NLP tasks.\n",
    "\n",
    "(You will learn about NLP tasks, and get hands-on experience with tokenization, cleaning, and normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr-ryxeRg4xh"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "### **Part A**\n",
    "1. Preprocessing Techniques\n",
    "    - Loading your Data\n",
    "    - Data Cleaning\n",
    "    - Tokenization\n",
    "    - Normalization\n",
    "2. Simple Sentiment Analysis\n",
    "    - Counting vs Context\n",
    "\n",
    "3. Unigrams and Bigrams\n",
    "\n",
    "    - DEMO: Calculate the frequency of unigrams and bigrams in a sample review.\n",
    "\n",
    "### **Part B**\n",
    "4. Embeddings\n",
    "    - What are Embeddings?\n",
    "    - Plotting Embeddings\n",
    "    - Similar Words\n",
    "\n",
    "\n",
    "  \n",
    "5. Neural Networks for Language Modeling\n",
    "\n",
    "    - Simple Neural Network Model: Using TensorFlow, create and train a model on the simple text to predict the sentiment.\n",
    "\n",
    "    -Exercise: modify or optimize the model parameters to see the difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4QkGebJh8rq"
   },
   "source": [
    "## **1. Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PCW692vVgx4K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv_gvNzkh_Tl"
   },
   "source": [
    "### **Loading and exploring the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItgMepjxgx4L"
   },
   "source": [
    "For your convenience, the data is already stored in the file _reviews.pkl_ which you can load read using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6Ph0z45eUsA",
    "outputId": "feaec6fc-cd56-4492-a658-2061fe9d40e8"
   },
   "outputs": [],
   "source": [
    "# #gdown code to get pickle file from drive link\n",
    "# !gdown 1NtvaBOXc-dFlpUww6KgIP0Tq9jGD03ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P62nh-Wngx4M",
    "outputId": "c73444c4-6d2c-4a60-e447-53af370bf1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('reviews.pkl', 'rb') as file:\n",
    "    train, _ = pickle.load(file)\n",
    "\n",
    "##### check the length and type of train\n",
    "print(len(train)) #length\n",
    "print(type(train)) #type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UVqTJ6Bgx4N"
   },
   "source": [
    "Each review is stored in a list. You can explore the contents of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qxdoAP6xgx4O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know why some guys from US, Georgia or even from Bulgaria have the courage to express feelings about something they don't understand at all. For those who did not watch this movie - watch it. Don't expect too much or don't put some frameworks just because this is Kosturica. Watch the movie without prejudice, try to understand the whole humor inside - people of Serbia DID actually getting married while Bil Clinton bomb their villages, gypsies in all Balkans are ALWAYS try to f*ck you up in any way they can, LOVE is always unexpected, pure and colorful, and Balkans are extremely creative. For those who claims this is a bad movie I can see only that the American's sh*t (like Meet Dave, Get Smart etc) are much much worse than a pure, frank Balkan humoristic love story movie as Promise me. The comment should be useful and on second place should represent the personal view of the writer. I think the movie is great and people watch it must give their respects to the director and story told inside. It is simple, but true. It is brutal, but gentle and makes you laugh to dead.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS7MnTv8gx4O"
   },
   "source": [
    "### ⏸ How many characters does the longest review has?\n",
    "\n",
    "#### A. 4000\n",
    "#### B. 10363\n",
    "#### C. 8754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rs0dQHuKYutf"
   },
   "outputs": [],
   "source": [
    "##check your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HtY7LYHwgx4O"
   },
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdecYn-7gx4P"
   },
   "source": [
    "The dataset is organized such as the first 2000 reviews are positive, and the rest negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wkZ-vwkvgx4P"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_pos \u001b[38;5;241m=\u001b[39m train[___] \u001b[38;5;66;03m#index first 2000\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_neg \u001b[38;5;241m=\u001b[39m train[___]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "train_pos = train[___] #index first 2000\n",
    "train_neg = train[___] #inex last 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju7l0748iFj9"
   },
   "source": [
    "### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e6bYXzkgx4P"
   },
   "source": [
    "Each review is in natural language, which means that it contains punctuation signs and uppercase letters. Because of the nature of the data, each review has HTML characters.\n",
    "\n",
    "In order to analyze the information better, our data cleaning involves removing unwanted characters, punctuations, and stopwords from the text.\n",
    "\n",
    "Let's use the following functions to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wapxa2TJijT8"
   },
   "source": [
    "#### **Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RufMG8fBgx4P"
   },
   "outputs": [],
   "source": [
    "def transform(review):\n",
    "    # Lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # Remove HTML\n",
    "    review = re.sub('<br /><br />', ' ', review)\n",
    "\n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    review = re.sub(r'[^a-zA-Z0\\s]', ' ', review)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysQOw4iFgx4P"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# HINT: use list comprehension to apply the transform function to each sentence\n",
    "train_pos = [___ for ___ in ___]\n",
    "train_neg = [___ for ___ in ___]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuXFA4F5ioij"
   },
   "source": [
    "#### **Removing stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJwEhMU13ohH",
    "outputId": "a41a9dcd-44cd-401c-e60d-7aa0743ebeec"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIzVKIEejAWc"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for sentence in train_pos:\n",
    "  cleaned_tokens = [word for word in sentence.split() if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsW5BaKY4OFm",
    "outputId": "68c4ffa8-be13-4925-fb77-4111f3ed99e5"
   },
   "outputs": [],
   "source": [
    "#print the cleane tokens to have a look\n",
    "cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JphU9n-cjLbi"
   },
   "source": [
    "### **Normalization**\n",
    "\n",
    "Normalization includes techniques like stemming and lemmatization to reduce words to their base or root form.\n",
    "\n",
    "#### Example: **Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjOdERD34hSa",
    "outputId": "83438e81-1cfd-4f27-e184-66c4110c0c74"
   },
   "outputs": [],
   "source": [
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvKnh8ypjOnO",
    "outputId": "932d727d-5605-47df-b12f-e3af2c934444"
   },
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize tokens\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFFN-sf7-qAA"
   },
   "source": [
    "##### You may notice that some words did not get properly lemmatized. This can be addressed by adding a **POS (Part of Speech)** Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlCtJS7s_-sq",
    "outputId": "cf8dbeb9-675b-4d4c-b671-992814fdd163"
   },
   "outputs": [],
   "source": [
    "# Download the WordNet resource like tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Create a new instance of WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample words and their part-of-speech tags\n",
    "tags = nltk.pos_tag(cleaned_tokens)\n",
    "\n",
    "# Convert POS tags to formats compatible with WordNetLemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default to noun\n",
    "\n",
    "# Lemmatizing words based on their POS tags\n",
    "lemmatized_words = {word: lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tags}\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TusbxfGSAmWT",
    "outputId": "d7730456-c6f7-44e1-e7d6-62f7ed368c9f"
   },
   "outputs": [],
   "source": [
    "# Create a PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Original Word\", \"Stemmed Word\"]\n",
    "\n",
    "# Adding rows to the table\n",
    "for original, lemma in lemmatized_words.items():\n",
    "    table.add_row([original, lemma])\n",
    "\n",
    "# Print the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNv6h9ay6VAw"
   },
   "source": [
    "#### **Stemming**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKBPS0gG6WQ3",
    "outputId": "d3896990-1791-4844-d8ed-6591990d08c5"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Applying stemming to each word\n",
    "stemmed_words = {word: stemmer.stem(word) for word in cleaned_tokens}\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPFeKFHni_go"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFLodjSHjC5o"
   },
   "source": [
    "## **2. Simple Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na4rTLPRgx4Q"
   },
   "source": [
    "In this exercise, you will look for keywords that you think represent a good movie and keywords that represent bad movies.\n",
    "\n",
    "We expect that a _positive_ review will have more possitive than negative words.\n",
    "\n",
    "To test the hypotesis, count the number of positive and negative words in each review of the training set, for each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQywY1Mjgx4Q"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "pos_key = ['good']#, 'excelent', 'great', 'cool']\n",
    "neg_key = ['bad']#, 'mediocre', 'regular', 'awful','terrible', 'boring', 'slow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyVXDlB0gx4Q"
   },
   "outputs": [],
   "source": [
    "# This counts the words for the positive reviews\n",
    "\n",
    "# Create a numpy array to store the scores\n",
    "scores_pos = np.zeros((len(train_pos), 2))\n",
    "\n",
    "# For each word in pos_key\n",
    "for j in range(len(pos_key)):\n",
    "    # For each review\n",
    "    for i in range(scores_pos.shape[0]):\n",
    "        # Add the number of times the word is found\n",
    "        scores_pos[i,0]+=train_pos[i].count(pos_key[j])\n",
    "\n",
    "# For each word in neg_key\n",
    "for j in range(len(neg_key)):\n",
    "    # For each review\n",
    "    for i in range(scores_pos.shape[0]):\n",
    "        # Add the number of times the word is found\n",
    "        scores_pos[i,1]+=train_pos[i].count(neg_key[j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVkxf_mwgx4Q"
   },
   "source": [
    "Using the same logic as above, compute the scores for the negative reviews next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrmIzEyKgx4Q"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "scores_neg = np.zeros((len(train_neg), 2))\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KevmQAuDgx4Q"
   },
   "source": [
    "### ⏸ What is the average number of appearances of the word **good** in the positive reviews?\n",
    "\n",
    "#### A. 0.30\n",
    "#### B. 0.45\n",
    "#### C. 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlxXiqfjYpmu"
   },
   "outputs": [],
   "source": [
    "##check your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl_ZSrpkgx4R"
   },
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UADsNhkXgx4R"
   },
   "source": [
    "In order to compare the success of your strategy, you must define a criteria.\n",
    "\n",
    "The simplest one to check if a review is good is to look if the number of positive words occurs more than negative words. For a negative review, the number of negative words should be higher than the positive ones.\n",
    "\n",
    "How many good reviews are classified correctly according to this score? How many negative ones are classified correctly?\n",
    "\n",
    "Remember that you are working with **2000** examples of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeXYhCECLcyY",
    "outputId": "7ea419d4-670b-4c6e-b78a-ababffb1eda1"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "b = scores_pos[:,0]>scores_pos[:,1]\n",
    "print('The number of positive reviews is',b.sum())\n",
    "\n",
    "b = scores_neg[:,0]<scores_neg[:,1]\n",
    "print('The number of negative reviews is',b.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLa6KZ5Qgx4R"
   },
   "source": [
    "Create **histograms** to analyze the score distribution.\n",
    "\n",
    "Do the results coincide with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "F6AH5drHgx4R",
    "outputId": "a6686a02-e807-4e23-b5a8-57d93112db4c"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "plt.hist(scores_pos[:,1], bins=[0,1,2,3,4,5,6,7,8,9,10], alpha=0.5, label='neg_key')\n",
    "plt.hist(scores_pos[:,0], bins=[0,1,2,3,4,5,6,7,8,9,10], alpha=0.5, label='pos_key')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNtekLsOgx4R"
   },
   "source": [
    "Generate the same histograms for the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "2kdozaURgx4R",
    "outputId": "9df8256d-2749-4504-a603-c04567fc156c"
   },
   "outputs": [],
   "source": [
    "plt.hist(scores_neg[:,1], bins=[0,1,2,3,4,5,6,7,8,9,10], alpha=0.5, label='neg_key')\n",
    "plt.hist(scores_neg[:,0], bins=[0,1,2,3,4,5,6,7,8,9,10], alpha=0.5, label='pos_key')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDfPceZVgx4S"
   },
   "source": [
    "In general, the presence of _bad_ words do not represent a bad movie. For example in the following review, we can see many _bad_ words that in the context are used to describe something positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "eM41tQDegx4S",
    "outputId": "5dd311cb-152f-4a7a-ca86-e9cb11873dea"
   },
   "outputs": [],
   "source": [
    "train[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55aPWymXgx4S"
   },
   "source": [
    "You can see that the words _bad_, _awful_ and _mediocre_ are in the review. But they are used to compare it to another title.\n",
    "\n",
    "Moreover, if you look closely, you will see that this is not a movie, but a game, which might have different keywords that describe its behavior.\n",
    "\n",
    "### ⏸ In your own words, How important is the context here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LSPXB_AFwTI"
   },
   "source": [
    "#### ##add your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lttNG1rwFz6U"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dXhsRwRK2G4"
   },
   "source": [
    "## 3. **Unigrams and Bigrams (DEMO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7BfwHPZ9K4iy",
    "outputId": "2ad54e56-9822-42a8-86af-49d88e2b0fbc"
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"This is a sample text for analysis. Text analysis helps visualize unigrams and bigrams.\"\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Calculate Unigrams frequency\n",
    "unigram_freq = FreqDist(tokens)\n",
    "\n",
    "# Calculate Bigrams frequency\n",
    "bigrams_tokens = list(bigrams(tokens))\n",
    "bigram_freq = FreqDist(bigrams_tokens)\n",
    "\n",
    "# Function to plot frequency distributions\n",
    "def plot_freq_dist(freq_dist, title=\"Frequency Distribution\", num=10):\n",
    "    most_common = freq_dist.most_common(num)\n",
    "    # Convert bigram tuples to strings explicitly\n",
    "    words = [str(item[0]) for item in most_common]  # Ensure words are strings\n",
    "    counts = [item[1] for item in most_common]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, counts, color='b')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Unigrams\n",
    "plot_freq_dist(unigram_freq, title=\"Unigram Frequency Distribution\")\n",
    "\n",
    "# Visualize Bigrams\n",
    "# For bigrams, need to convert tuple to string for plotting\n",
    "bigram_labels = [' '.join(bigram) for bigram in bigram_freq]\n",
    "plot_freq_dist(bigram_freq, title=\"Bigram Frequency Distribution\", num=len(bigram_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
